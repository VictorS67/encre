import {
  APIConnectionTimeoutError,
  APIUserAbortError,
  OpenAI as OpenAIClient,
} from 'openai';
import type { RequestOptions as OpenAIClientRequestOptions } from 'openai/core';

import { BaseLMCallOptions } from '../../base.js';

/**
 * Represents a call to a function as part of an OpenAI tool usage.
 * @example
 * ```typescript
 * const toolCall: OpenAIChatToolCall = {
 *   id: 'tool-id-from-api',
 *   type: 'function',
 *   function: {
 *     name: 'get_current_weather',
 *     arguments: "{\n\"location\": \"Boston, MA\"\n}"
 *   }
 * };
 * ```
 */
export interface OpenAIChatToolCall {
  /**
   * The ID of the tool call.
   */
  id: string;

  /**
   * The function that the model called.
   */
  function: {
    /**
     * The arguments to call the function with, as generated by the model in JSON
     * format. Note that the model does not always generate valid JSON, and may
     * hallucinate parameters not defined by your function schema. Validate the
     * arguments in your code before calling your function.
     */
    arguments: string;

    /**
     * The name of the function to call.
     */
    name: string;
  };

  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: 'function';
}

/**
 * Specifies a tool the model should use. Use to force the model to call a specific
 * function.
 * @example
 * ```typescript
 * const toolChoice: OpenAIChatToolChoice = {
 *   type: 'function',
 *   function: {
 *     name: 'get_current_weather'
 *   }
 * };
 * ```
 */
export interface OpenAIChatToolChoice {
  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: 'function';

  function: {
    /**
     * The name of the function to call.
     */
    name: string;
  };
}

/**
 * Represents a tool available for the model to use during processing.
 * @example
 * ```typescript
 * const tool: OpenAIChatTool = {
 *   type: 'function',
 *   function: {
 *     name: 'get_current_weather',
 *     description: 'get weather in a given location',
 *     parameters: {
 *       type: 'object',
 *       properties: {
 *         location: {
 *           type: 'string'
 *         },
 *         unit: {
 *           type: 'string',
 *           enum: ['celsius', 'fahrenheit'],
 *         }
 *       },
 *       required: ['location']
 *     }
 *   }
 * };
 * ```
 */
export interface OpenAIChatTool {
  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: 'function';

  function: OpenAIFunctionDef;
}

/**
 * Details of a function that can be called by the model.
 * @example
 * ```typescript
 * const functionDef: OpenAIFunctionDef = {
 *   name: 'get_current_weather',
 *   description: 'get weather in a given location',
 *   parameters: {
 *     type: 'object',
 *     properties: {
 *       location: {
 *         type: 'string'
 *       },
 *       unit: {
 *         type: 'string',
 *         enum: ['celsius', 'fahrenheit'],
 *       }
 *     },
 *     required: ['location']
 *   }
 * };
 * ```
 */
export interface OpenAIFunctionDef {
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
   * underscores and dashes, with a maximum length of 64.
   */
  name: string;

  /**
   * A description of what the function does, used by the model to choose when and
   * how to call the function.
   */
  description?: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the
   * [guide]({@link https://platform.openai.com/docs/guides/text-generation/function-calling})
   * for examples, and the
   * [JSON Schema reference]({@link https://json-schema.org/understanding-json-schema/}) for
   * documentation about the format.
   *
   * Omitting `parameters` defines a function with an empty parameter list.
   */
  parameters?: Record<string, unknown>;
}

/**
 * Enhanced options for OpenAI chat-related API calls.
 */
export interface OpenAIChatCallOptions extends OpenAICallOptions {
  /**
   * Whether to return log probabilities of the output tokens or not. If true,
   * returns the log probabilities of each output token returned in the content
   * of message. This option is currently not available on the `gpt-4-vision-preview` model.
   */
  logprobs?: boolean;

  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to
   * return at each token position, each with an associated log probability.
   * `logprobs` must be set to `true` if this parameter is used.
   */
  topLogprobs?: number;

  /**
   * Controls which (if any) function is called by the model. `none` means the model
   * will not call a function and instead generates a message. `auto` means the model
   * can pick between generating a message or calling a function. Specifying a
   * particular function via
   * `{"type": "function", "function": {"name": "my_function"}}` forces the model to
   * call that function.
   *
   * `none` is the default when no functions are present. `auto` is the default if
   * functions are present.
   */
  toolChoice?: 'none' | 'auto' | OpenAIChatToolChoice;

  /**
   * A list of tools the model may call. Currently, only functions are supported as a
   * tool. Use this to provide a list of functions the model may generate JSON inputs
   * for. A max of 128 functions are supported.
   */
  tools?: Array<OpenAIChatTool>;
}

/**
 * Options specific to text-based API calls.
 */
export interface OpenAITextCallOptions extends OpenAICallOptions {
  /**
   * The suffix that comes after a completion of inserted text.
   *
   * This parameter is only supported for model `gpt-3.5-turbo-instruct`.
   */
  suffix?: string;
}

/**
 * Base options for any OpenAI API call.
 */
export interface OpenAICallOptions extends BaseLMCallOptions {
  /**
   * Additional options to pass to the underlying axios request.
   */
  options?: OpenAIClientRequestOptions;
}

/**
 * Core settings for interacting with OpenAI API.
 */
export interface OpenAIBaseInput {
  /**
   * ID of the model to use. You can use the [List models]({@link https://platform.openai.com/docs/api-reference/models/list})
   * API to see all of your available models, or see [Model overview]({@link https://platform.openai.com/docs/models/overview})
   * for descriptions of them.
   */
  modelName: string;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
   * existing frequency in the text so far, decreasing the model's likelihood to
   * repeat the same line verbatim.
   *
   * [See more information about frequency and presence panalties]({@link https://platform.openai.com/docs/guides/text-generation/parameter-details})
   */
  frequencyPenalty: number;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   * whether they appear in the text so far, increasing the model's likelihood to
   * talk about new topics.
   *
   * [See more information about frequency and presence penalties]({@link https://platform.openai.com/docs/guides/text-generation/parameter-details})
   */
  presencePenalty: number;

  /**
   * Whether to stream back partial progress. If set, tokens will be sent as
   * data-only [server-sent events]({@link https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format})
   * as they become available, with the stream terminated by a `data: [DONE]`
   * message.
   *
   * [Example Python code]({@link https://cookbook.openai.com/examples/how_to_stream_completions})
   */
  streaming: boolean;

  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
   * make the output more random, while lower values like 0.2 will make it more
   * focused and deterministic.
   *
   * We generally recommend altering this or `topP` but not both.
   */
  temperature: number;

  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the
   * model considers the results of the tokens with temperature probability mass. So 0.1
   * means only the tokens comprising the top 10% probability mass are considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   */
  topP: number;

  /**
   * How many completions to generate for each prompt.
   *
   * **Note:** Because this parameter generates many completions, it can quickly
   * consume your token quota. Use carefully and ensure that you have reasonable
   * settings for `maxTokens` and `stopWords`.
   */
  n: number;

  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * Accepts a json object that maps tokens (specified by their token ID in the GPT
   * tokenizer) to an associated bias value from -100 to 100. You can use this
   * [tokenizer tool]({@link https://platform.openai.com/tokenizer?view=bpe})
   * (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically,
   * the bias is added to the logits generated by the model prior to sampling.
   * The exact effect will vary per model, but values between -1 and 1 should
   * decrease or increase likelihood of selection; values like -100 or 100 should
   * result in a ban or exclusive selection of the relevant token.
   *
   * As an example, you can pass `{"50256": -100}` to prevent the end-of-text token
   * from being generated.
   */
  logitBias?: Record<string, number>;

  /**
   * The maximum number of [tokens]({@link https://platform.openai.com/tokenizer})
   * to generate in the completion.
   *
   * The token count of your prompt plus `maxTokens` cannot exceed the model's
   * context length.
   *
   * [Example Python code]({@link https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken}) f
   * or counting tokens.
   */
  maxTokens?: number;

  /**
   * This feature is in Beta. If specified, our system will make a best effort to
   * sample deterministically, such that repeated requests with the same `seed` and
   * parameters should return the same result. Determinism is not guaranteed, and you
   * should refer to the `system_fingerprint` response parameter to monitor changes
   * in the backend.
   */
  seed?: number;

  /**
   * Up to 4 sequences where the API will stop generating further tokens. The
   * returned text will not contain the stop sequence.
   */
  stopWords?: string[];

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor
   * and detect abuse.
   *
   * [Learn more]({@link https://platform.openai.com/docs/guides/safety-best-practices})
   */
  user?: string;

  /**
   * Holds any additional parameters that are valid to pass to
   * [OpenAI API Reference]({@link https://platform.openai.com/docs/api-reference/introduction})
   * that are not explicitly specified on this class.
   */
  additionalKwargs?: Record<string, unknown>;

  /**
   * Timeout to use when making requests to OpenAI.
   */
  timeout?: number;

  /**
   * API key to use when making requests to OpenAI. Defaults to the value of
   * `OPENAI_API_KEY` environment variable.
   */
  openAIApiKey?: string;
}


/**
 * Configuration for standard OpenAI API input parameters.
 */
export interface OpenAIInput extends OpenAIBaseInput {
  /**
   * Generates `bestOf` completions server-side and returns the "best" (the one with
   * the highest log probability per token). Results cannot be streamed.
   *
   * When used with `n`, `bestOf` controls the number of candidate completions and
   * `n` specifies how many to return – `bestOf` must be greater than `n`.
   *
   * **Note:** Because this parameter generates many completions, it can quickly
   * consume your token quota. Use carefully and ensure that you have reasonable
   * settings for `maxToken` and `stopWords`.
   */
  bestOf?: number;

  /**
   * Include the log probabilities on the `logprobs` most likely tokens, as well the
   * chosen tokens. For example, if `logprobs` is 5, the API will return a list of
   * the 5 most likely tokens. The API will always return the `logprob` of the
   * sampled token, so there may be up to `logprobs+1` elements in the response.
   *
   * The maximum value for `logprobs` is 5.
   */
  logprobs?: number;

  /**
   * Echo back the prompt in addition to the completion
   */
  echo?: boolean;
}

/**
 * Configuration for chat-specific input settings in OpenAI API calls.
 */
export interface OpenAIChatInput extends OpenAIBaseInput {
  /**
   * An object specifying the format that the model must output. Compatible with
   * [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and
   * all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
   *
   * Setting to `"json"` enables JSON mode, which guarantees the message the model
   * generates is valid JSON.
   *
   * **Important:** when using JSON mode, you **must** also instruct the model to
   * produce JSON yourself via a system or user message. Without this, the model may
   * generate an unending stream of whitespace until the generation reaches the token
   * limit, resulting in a long-running and seemingly "stuck" request. Also note that
   * the message content may be partially cut off if `finish_reason="length"`, which
   * indicates the generation exceeded `maxTokens` or the conversation exceeded the
   * max context length.
   */
  responseFormatType?: 'text' | 'json';

  /** ChatGPT messages to pass as a prefix to the prompt */
  chatMessages?: OpenAIClient.Chat.ChatCompletionMessageParam[];
}

/**
 * Checks if a model name is suitable for OpenAI chat functions.
 * @param modelName The model name to check.
 * @returns True if the model is compatible with chat functions.
 */
export function checkModelForOpenAIChat(modelName?: string): boolean {
  return (
    modelName !== undefined &&
    (modelName.startsWith('gpt-3.5-turbo') || modelName.startsWith('gpt-4')) &&
    !modelName.includes('-instruct')
  );
}

/**
 * Checks if a model name is suitable for OpenAI vision-related tasks.
 * @param modelName The model name to check.
 * @returns True if the model is suitable for vision tasks.
 */
export function checkModelForOpenAIVision(modelName?: string): boolean {
  return (
    modelName !== undefined &&
    modelName.startsWith('gpt-4') &&
    modelName.includes('vision')
  );
}

/**
 * Wraps errors from the OpenAI client to standardize error handling.
 * @param e The original error thrown by the OpenAI client.
 * @returns A standardized error object.
 */
export function wrapOpenAIClientError(e: Error): Error {
  let error: Error;
  if (e.constructor.name === APIConnectionTimeoutError.name) {
    error = new Error(e.message);
    error.name = 'TimeoutError';
  } else if (e.constructor.name === APIUserAbortError.name) {
    error = new Error(e.message);
    error.name = 'AbortError';
  } else {
    error = e;
  }

  return error;
}
